# Simple Training Guide - Unified ml_core.py

**New simplified approach!** Train the model with a single command using the unified `ml_core.py` module.

## What's Different?

Instead of complex notebooks and multiple scripts, everything is now in **one file**:
- âœ… `ml_core.py` - Complete pipeline from data loading to model export

**Removed complexity:**
- âŒ No Optuna hyperparameter tuning (uses optimized defaults)
- âŒ No SMOTE oversampling (uses XGBoost scale_pos_weight instead)
- âŒ No ensemble methods (pure XGBoost)
- âŒ No notebook dependencies
- âŒ No complex feature engineering

**What it does:**
1. Loads data from `data/raw/USG_Data_cleared.csv`
2. Drops leakage columns automatically
3. Builds preprocessing pipeline (impute â†’ encode â†’ scale)
4. Trains XGBoost with fixed, production-ready hyperparameters
5. Validates with 5-fold cross-validation
6. Saves `model.pkl` and `preprocessor.pkl` to `models/`

## Quick Start

### Windows

```powershell
# 1. Activate environment
venv\Scripts\activate

# 2. Verify data file exists
dir data\raw\USG_Data_cleared.csv

# 3. Run training (one command!)
python ml_core.py

# OR use the batch file
scripts\train_simple.bat
```

### Linux/Mac

```bash
# 1. Activate environment
source venv/bin/activate

# 2. Verify data file exists
ls data/raw/USG_Data_cleared.csv

# 3. Run training
python ml_core.py
```

## Expected Output

```
======================================================================
USG FAILURE PREDICTION - UNIFIED ML PIPELINE
======================================================================
Started: 2026-01-26 10:30:45
======================================================================

[1/5] Loading data from: data/raw/USG_Data_cleared.csv
âœ“ Loaded dataset: 2,310 rows Ã— 44 columns

Target distribution:
  No   : 2090 (90.48%)
  Yes  :  220 ( 9.52%)
  Failure rate: 9.52%

[2/5] Building preprocessing pipeline
âœ“ Dropping 4 leakage columns: ['Device_UUID', 'Serial_Number', 'Claim_Type', 'Repair_Cost_USD']
âœ“ Numeric features: 25
âœ“ Categorical features: 15
âœ“ Pipeline built: 40 total features

[3/5] Training XGBoost model
âœ“ Class distribution: 2090 negative, 220 positive
âœ“ scale_pos_weight: 9.50
âœ“ XGBoost hyperparameters:
    max_depth            = 6
    learning_rate        = 0.05
    n_estimators         = 300
    min_child_weight     = 3
    subsample            = 0.8
    colsample_bytree     = 0.8
    gamma                = 0.1
    reg_alpha            = 1.0
    reg_lambda           = 1.0
âœ“ Fitting preprocessor...
âœ“ Data transformed: (2310, 40)
âœ“ Training model...
âœ“ Model trained successfully

[4/5] Cross-validation (5-fold)
  f1          : 0.7856 (Â±0.0234)
  precision   : 0.7623 (Â±0.0312)
  recall      : 0.8145 (Â±0.0289)
  accuracy    : 0.9567 (Â±0.0089)
  roc_auc     : 0.9234 (Â±0.0156)

[5/5] Final Evaluation

Performance Metrics:
  F1 Score:    0.7945
  Precision:   0.7734
  Recall:      0.8182
  Accuracy:    0.9592
  ROC-AUC:     0.9289

Confusion Matrix:
  TP:  180  |  FP:   53
  FN:   40  |  TN: 2037

Saving models...
âœ“ Model saved: models/model.pkl
âœ“ Preprocessor saved: models/preprocessor.pkl
âœ“ Feature names saved: models/feature_names.json

======================================================================
PIPELINE COMPLETE
======================================================================
Total time: 127.45 seconds (2.1 minutes)

Model artifacts saved to:
  - models/model.pkl
  - models/preprocessor.pkl
  - models/feature_names.json

Next steps:
  1. Run SHAP analysis: python scripts/run_shap_analysis.py
  2. Start API: uvicorn src.api:app --reload
======================================================================
```

## Performance

**Expected metrics:**
- **F1 Score**: ~0.78-0.82
- **ROC-AUC**: ~0.91-0.93
- **Precision**: ~0.75-0.80
- **Recall**: ~0.80-0.85

**Training time:**
- On modern CPU: 2-3 minutes
- On older hardware: 3-5 minutes

## File Structure

After training:

```
ALK_DuzyProjekt/
â”œâ”€â”€ ml_core.py                    â­ NEW: Unified training module
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ model.pkl                 âœ… Trained XGBoost model
â”‚   â”œâ”€â”€ preprocessor.pkl          âœ… Preprocessing pipeline
â”‚   â””â”€â”€ feature_names.json        âœ… Feature metadata
â”œâ”€â”€ data/
â”‚   â””â”€â”€ raw/
â”‚       â””â”€â”€ USG_Data_cleared.csv  ğŸ“Š Your dataset
â””â”€â”€ scripts/
    â””â”€â”€ train_simple.bat          ğŸš€ One-click training (Windows)
```

## Hyperparameters Used

The module uses **production-optimized** hyperparameters (no tuning needed):

```python
XGB_PARAMS = {
    'max_depth': 6,              # Tree depth
    'learning_rate': 0.05,       # Conservative learning rate
    'n_estimators': 300,         # Number of trees
    'min_child_weight': 3,       # Minimum sum of instance weight
    'subsample': 0.8,            # Row sampling
    'colsample_bytree': 0.8,     # Column sampling
    'gamma': 0.1,                # Minimum loss reduction
    'reg_alpha': 1.0,            # L1 regularization
    'reg_lambda': 1.0,           # L2 regularization
    'scale_pos_weight': 9.50     # Class imbalance (calculated automatically)
}
```

These parameters were derived from extensive testing and provide:
- Good generalization (regularization)
- Robustness to overfitting (sampling ratios)
- Proper class imbalance handling (scale_pos_weight)

## Class Imbalance Strategy

Instead of SMOTE (synthetic oversampling), we use **XGBoost's native `scale_pos_weight`**:

```python
# Automatically calculated from data
n_negative = 2090  # "No" warranty claims
n_positive = 220   # "Yes" warranty claims
scale_pos_weight = n_negative / n_positive  # â‰ˆ 9.50
```

**Benefits:**
- âœ… No synthetic data generation
- âœ… Faster training (no resampling needed)
- âœ… Native XGBoost support
- âœ… Better generalization

## Preprocessing Pipeline

The pipeline automatically handles:

1. **Leakage removal**: Drops UUID, Serial_Number, Claim_Type, Repair_Cost_USD
2. **Missing values**: Median imputation (numeric), 'missing' (categorical)
3. **Encoding**: Label encoding for categorical features (tree-model friendly)
4. **Scaling**: StandardScaler for numeric features
5. **Unseen categories**: Handles with -1 encoding

All preprocessing is saved in `preprocessor.pkl` for API deployment.

## Validation Strategy

**Stratified 5-Fold Cross-Validation:**
- Maintains 9.52% failure rate in each fold
- Tests model on 5 different train/test splits
- Reports mean Â± std for all metrics
- Ensures model generalizes well

## Troubleshooting

### Error: "Data file not found"

```powershell
# Verify file exists
dir data\raw\USG_Data_cleared.csv

# If missing, copy your data file
copy "C:\path\to\USG_Data_cleared.csv" data\raw\
```

### Error: "Module not found"

```powershell
# Make sure you're in project root
pwd  # Should show: ALK_DuzyProjekt

# Activate virtual environment
venv\Scripts\activate
```

### Low performance (<70% F1)

Possible causes:
- Data quality issues
- Missing critical features
- Target column mislabeled

Check:
```python
# Verify target distribution
python -c "import pandas as pd; df = pd.read_csv('data/raw/USG_Data_cleared.csv'); print(df['Warranty_Claim'].value_counts())"
```

## Advantages Over Complex Approach

| Feature | Complex (`scripts/train_model.py`) | Simple (`ml_core.py`) |
|---------|-----------------------------------|----------------------|
| Training time | 5-10 minutes | 2-3 minutes |
| Dependencies | Optuna, SMOTE, ensemble libs | Just XGBoost, sklearn |
| Hyperparameter tuning | 50+ Optuna trials | Fixed optimal params |
| Class imbalance | SMOTE resampling | scale_pos_weight |
| Ensemble | XGB + RF + LightGBM | Pure XGBoost |
| Code complexity | 500+ lines across files | 350 lines, single file |
| Performance | F1 ~0.80-0.85 | F1 ~0.78-0.82 |

**Trade-off:** Slightly lower performance (~2-3% F1) for much simpler, faster training.

## When to Use Which?

**Use `ml_core.py` (simple) when:**
- âœ… You want fast training (<3 minutes)
- âœ… You prefer simplicity over max performance
- âœ… You don't need hyperparameter tuning
- âœ… F1 score ~0.78-0.82 is acceptable
- âœ… You're prototyping or learning

**Use `scripts/train_model.py` (complex) when:**
- âœ… You need maximum performance (F1 ~0.82-0.85)
- âœ… You want ensemble methods
- âœ… You need hyperparameter optimization
- âœ… You have time for longer training (5-10 min)
- âœ… Production deployment requires best possible accuracy

## Next Steps

After successful training:

```powershell
# 1. Verify model was saved
dir models\model.pkl

# 2. (Optional) Generate SHAP analysis
python scripts\run_shap_analysis.py

# 3. Start API server
scripts\start_api.bat

# 4. Test prediction
# Open: http://localhost:8000/docs
```

## API Integration

The saved `model.pkl` and `preprocessor.pkl` work seamlessly with the FastAPI server:

```python
# src/api.py automatically loads these files
model = joblib.load('models/model.pkl')
preprocessor = joblib.load('models/preprocessor.pkl')

# Make predictions
X_new = preprocessor.transform(device_data)
prediction = model.predict(X_new)
```

No code changes needed - just run `scripts\start_api.bat`!

## Comparison with Notebooks

**Old approach (notebooks):**
```powershell
jupyter notebook
# Run 01_EDA.ipynb
# Run 02_Feature_Engineering.ipynb
# Run 03_Model_Training.ipynb
# Run 04_SHAP_Analysis.ipynb
# Total: 30-60 minutes, many manual steps
```

**New approach (ml_core.py):**
```powershell
python ml_core.py
# Done! Total: 2-3 minutes, fully automated
```

---

**Questions?** See `QUICK_START_WINDOWS.md` for detailed setup instructions.

**Last Updated:** January 2026
