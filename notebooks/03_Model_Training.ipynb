{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USG Failure Prediction - Model Training & Optimization\n",
    "\n",
    "**Objective:** Train and optimize XGBoost model with ensemble methods\n",
    "\n",
    "**Techniques:**\n",
    "- Hyperparameter tuning with Optuna (50+ trials)\n",
    "- SMOTE for class imbalance\n",
    "- Ensemble with Random Forest & LightGBM\n",
    "- Probability calibration\n",
    "- Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import joblib\n",
    "import json\n",
    "\n",
    "# Import custom modules\n",
    "from model import USGFailurePredictionModel, XGBoostOptimizer\n",
    "from evaluation import ModelEvaluator\n",
    "from preprocessing import USGPreprocessingPipeline\n",
    "\n",
    "# ML libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Configure\n",
    "np.random.seed(42)\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "%matplotlib inline\n",
    "\n",
    "print(f\"Model Training started: {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Load pre-processed data if available\n",
    "try:\n",
    "    X = pd.read_csv('../data/processed/X_processed.csv')\n",
    "    y = pd.read_csv('../data/processed/y_target.csv').squeeze()\n",
    "    print(f\"✓ Loaded processed data: X={X.shape}, y={y.shape}\")\n",
    "    preprocessor = joblib.load('../models/preprocessor.pkl')\n",
    "    print(\"✓ Loaded preprocessor\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    # Option 2: Load raw data and preprocess\n",
    "    print(\"Processed data not found. Loading and preprocessing raw data...\")\n",
    "    \n",
    "    df = pd.read_csv('../data/raw/USG_Data_cleared.csv')\n",
    "    \n",
    "    if 'Warranty_Claim' in df.columns:\n",
    "        X_raw = df.drop('Warranty_Claim', axis=1)\n",
    "        y = df['Warranty_Claim']\n",
    "        \n",
    "        # Preprocess\n",
    "        preprocessor = USGPreprocessingPipeline(seed=42)\n",
    "        X = preprocessor.fit_transform(X_raw, y)\n",
    "        \n",
    "        # Save\n",
    "        X.to_csv('../data/processed/X_processed.csv', index=False)\n",
    "        y.to_csv('../data/processed/y_target.csv', index=False)\n",
    "        joblib.dump(preprocessor, '../models/preprocessor.pkl')\n",
    "        \n",
    "        print(f\"✓ Data processed and saved: X={X.shape}, y={y.shape}\")\n",
    "\n",
    "# Display target distribution\n",
    "print(\"\\nTarget Distribution:\")\n",
    "print(y.value_counts())\n",
    "print(f\"Failure rate: {(y == 'Yes').mean() * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified split to maintain class balance\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "print(f\"\\nTrain target distribution:\")\n",
    "print(y_train.value_counts())\n",
    "print(f\"\\nTest target distribution:\")\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Hyperparameter Optimization with Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize optimizer\n",
    "optimizer = XGBoostOptimizer(\n",
    "    n_trials=50,  # Increase for better results (100+)\n",
    "    cv_folds=5,\n",
    "    seed=42,\n",
    "    use_smote=True\n",
    ")\n",
    "\n",
    "print(\"Starting hyperparameter optimization...\")\n",
    "print(\"This may take 3-5 minutes...\\n\")\n",
    "\n",
    "# Run optimization\n",
    "best_params = optimizer.optimize(X_train, y_train)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"OPTIMAL HYPERPARAMETERS\")\n",
    "print(\"=\"*60)\n",
    "for param, value in best_params.items():\n",
    "    print(f\"{param:20s}: {value}\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Best CV F1 Score: {optimizer.study.best_value:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize optimization history\n",
    "import optuna.visualization as vis\n",
    "\n",
    "fig = vis.plot_optimization_history(optimizer.study)\n",
    "fig.write_html('../reports/visualizations/optuna_optimization_history.html')\n",
    "fig.show()\n",
    "\n",
    "print(\"✓ Optimization history saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train Full Model with Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model with best parameters\n",
    "model = USGFailurePredictionModel(\n",
    "    optimize_hyperparams=False,  # Use pre-optimized params\n",
    "    use_ensemble=True,\n",
    "    use_calibration=True,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Set best parameters\n",
    "model.best_params = best_params\n",
    "\n",
    "# Train\n",
    "print(\"Training ensemble model...\")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(f\"\\n✓ Model training complete in {model.training_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize evaluator\n",
    "evaluator = ModelEvaluator(seed=42)\n",
    "\n",
    "# Comprehensive evaluation\n",
    "print(\"Running comprehensive evaluation...\\n\")\n",
    "\n",
    "evaluation_results = evaluator.evaluate(\n",
    "    model=model,\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    run_stress_tests=True,\n",
    "    run_fairness_check=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display key metrics\n",
    "test_metrics = evaluation_results['test_metrics']\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST SET PERFORMANCE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"F1 Score:      {test_metrics['f1_score']:.4f}\")\n",
    "print(f\"Precision:     {test_metrics['precision']:.4f}\")\n",
    "print(f\"Recall:        {test_metrics['recall']:.4f}\")\n",
    "print(f\"ROC-AUC:       {test_metrics['roc_auc']:.4f}\")\n",
    "print(f\"PR-AUC:        {test_metrics['pr_auc']:.4f}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Confusion matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(f\"  TP: {test_metrics['true_positives']:4d}  |  FP: {test_metrics['false_positives']:4d}\")\n",
    "print(f\"  FN: {test_metrics['false_negatives']:4d}  |  TN: {test_metrics['true_negatives']:4d}\")\n",
    "print(f\"\\nBusiness Cost: ${test_metrics['business_cost']:,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation results\n",
    "if 'cv_results' in evaluation_results:\n",
    "    cv = evaluation_results['cv_results']\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"CROSS-VALIDATION RESULTS (5-Fold)\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"F1:        {cv['f1_mean']:.4f} (+/- {cv['f1_std']:.4f})\")\n",
    "    print(f\"Precision: {cv['precision_mean']:.4f} (+/- {cv['precision_std']:.4f})\")\n",
    "    print(f\"Recall:    {cv['recall_mean']:.4f} (+/- {cv['recall_std']:.4f})\")\n",
    "    print(f\"ROC-AUC:   {cv['roc_auc_mean']:.4f} (+/- {cv['roc_auc_std']:.4f})\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Threshold Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot threshold analysis\n",
    "if 'threshold_analysis' in evaluation_results:\n",
    "    threshold_results = evaluation_results['threshold_analysis']\n",
    "    \n",
    "    print(\"\\nOptimal Operating Point:\")\n",
    "    print(f\"  Threshold:  {threshold_results['optimal_threshold']:.4f}\")\n",
    "    print(f\"  F1 Score:   {threshold_results['optimal_f1']:.4f}\")\n",
    "    print(f\"  Precision:  {threshold_results['optimal_precision']:.4f}\")\n",
    "    print(f\"  Recall:     {threshold_results['optimal_recall']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Model and Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model.save('../models/model.pkl')\n",
    "print(\"✓ Model saved to models/model.pkl\")\n",
    "\n",
    "# Save feature names\n",
    "feature_names = X.columns.tolist()\n",
    "with open('../models/feature_names.json', 'w') as f:\n",
    "    json.dump(feature_names, f, indent=2)\n",
    "print(\"✓ Feature names saved to models/feature_names.json\")\n",
    "\n",
    "# Save evaluation results\n",
    "evaluator.save_results('../reports/metrics/evaluation_results.json')\n",
    "print(\"✓ Evaluation results saved to reports/metrics/evaluation_results.json\")\n",
    "\n",
    "# Save best hyperparameters\n",
    "with open('../reports/metrics/best_hyperparameters.json', 'w') as f:\n",
    "    json.dump(best_params, f, indent=2)\n",
    "print(\"✓ Best hyperparameters saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL TRAINING SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nModel Type: XGBoost Ensemble (XGB + RF + LightGBM)\")\n",
    "print(f\"Features: {X.shape[1]}\")\n",
    "print(f\"Training Samples: {X_train.shape[0]}\")\n",
    "print(f\"Test Samples: {X_test.shape[0]}\")\n",
    "print(f\"Training Time: {model.training_time:.2f} seconds\")\n",
    "\n",
    "print(f\"\\nOptimization:\")\n",
    "print(f\"  - Optuna trials: {optimizer.n_trials}\")\n",
    "print(f\"  - Best CV F1: {optimizer.study.best_value:.4f}\")\n",
    "print(f\"  - SMOTE: Enabled\")\n",
    "print(f\"  - Calibration: Platt Scaling\")\n",
    "\n",
    "print(f\"\\nTest Performance:\")\n",
    "print(f\"  - F1 Score: {test_metrics['f1_score']:.4f}\")\n",
    "print(f\"  - ROC-AUC: {test_metrics['roc_auc']:.4f}\")\n",
    "print(f\"  - PR-AUC: {test_metrics['pr_auc']:.4f}\")\n",
    "print(f\"  - Business Cost: ${test_metrics['business_cost']:,.2f}\")\n",
    "\n",
    "print(\"\\nNext Steps:\")\n",
    "print(\"  → SHAP interpretability analysis (Notebook 04)\")\n",
    "print(\"  → Deploy via FastAPI (src/api.py)\")\n",
    "print(\"  → Generate business report\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"Training completed: {datetime.now()}\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
